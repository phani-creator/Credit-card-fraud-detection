{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Import psql to run queries \n",
    "\n",
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(r\"C:\\Users\\phani\\OneDrive\\Documents\\Machine learning internship\\Project\\cards.csv\", header=0)\n",
    "\n",
    "# Copy the file to back-up file\n",
    "\n",
    "m_bk = m.copy()\n",
    "\n",
    "# display first 5 records\n",
    "\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=['used_chip','used_pin_number']\n",
    "m=pd.get_dummies(m,columns=cat)\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndepVar = []\n",
    "for col in m.columns:\n",
    "    if col != 'fraud':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'fraud'\n",
    "\n",
    "x = m[IndepVar]\n",
    "y = m[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify=y, random_state=42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train = mmscaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "\n",
    "x_test = mmscaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Results = pd.read_csv(r\"C:\\Users\\phani\\OneDrive\\Documents\\Machine learning internship\\Project\\cardsresult.csv\", header=0)\n",
    "Results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the 'Logistic Regression' model with random sampling\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an object for model\n",
    "\n",
    "ModelLR = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ModelLR.fit(x_train, y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y_pred = ModelLR.predict(x_test)\n",
    "y_pred_prob = ModelLR.predict_proba(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# Predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "# Outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# Classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n', C_Report)\n",
    "\n",
    "# Calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3);\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "if(mx<0):\n",
    "    mx=-mx\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Specificity or True Negative Rate :', round(specificity*100,2), '%')\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))\n",
    "\n",
    "# ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,ModelLR.predict_proba(x_test)[:,1])\n",
    "plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "\n",
    "plt.plot(fpr, tpr, label= 'Classification Model' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "# To build the 'Multinominal Decision Tree' model with random sampling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ModelRF = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                 max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "                                 ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "# Train the model with train data \n",
    "\n",
    "ModelRF.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y_pred = ModelRF.predict(x_test)\n",
    "y_pred_prob = ModelRF.predict_proba(x_test)\n",
    "\n",
    "# Confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n', C_Report)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3);\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the SVM algorithm with train dataset\n",
    "\"\"\"\"from sklearn.svm import SVC\n",
    "bankdataSVM = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True,\n",
    "                  probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                  max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "# Train the model with train data \n",
    "bankdataSVM = bankdataSVM.fit(x_train, y_train)\n",
    "# Predict the model with test data set\n",
    "y_pred = bankdataSVM.predict(x_test)\n",
    "y_pred_prob = bankdataSVM.predict_proba(x_test)\n",
    "# Confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# actual values\n",
    "actual = y_test\n",
    "# predicted values\n",
    "predicted = y_pred\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n', C_Report)\n",
    "# calculating the metrics\n",
    "sensitivity = round(tp/(tp+fn), 3);\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "from math import sqrt\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Specificity or True Negative Rate :', round(specificity*100,2),'%' )\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "# Area under ROC curve \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "model_roc_auc = roc_auc_score(actual, predicted)\n",
    "fpr, tpr, thresholds = roc_curve(actual,bankdataSVM.predict_proba(x_test)[:,1])\n",
    "plt.figure()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
